{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tevfikaytekin/data_science/blob/master/recommender_systems/matrix_factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgs5IIoUSn9l"
   },
   "source": [
    "# Matrix Factorization\n",
    "(by Tevfik Aytekin)\n",
    "\n",
    "Matrix factorizarion is one of the state-of-the-art techniques used in recommender systems. Below you can find several different implementations.\n",
    "\n",
    "Over the years many variations of matrix factorization have been proposed. The following formulation is one of the simplest but which works well as we will see. It can be extended it various ways, see for example [Advances in Collaborative Filtering](https://link.springer.com/chapter/10.1007/978-1-4899-7637-6_3).\n",
    "\n",
    "Cost Function:\n",
    "$$\n",
    "J(\\Theta) =  \\sum_{u,i \\in K} (r_{ui} - q^T_ip_u)^2 + \\lambda(||q_i||^2+||p_u||^2)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $r_{ui}$ is the rating of user $u$ for item $i$.\n",
    "- $K$ is the set of $(u,i)$ pairs for which $r_{ui}$ is known.\n",
    "- $q_i$, $p_u$ are latent factor vectors for items and users, respectively. \n",
    "- $\\lambda$ is the regularization parameter.\n",
    "\n",
    "And the optimization objective:\n",
    "\n",
    "$$\n",
    "\\min_{p*,q*} \\sum_{u,i \\in K} (r_{ui} - q^T_ip_u)^2 + \\lambda(||q_i||^2+||p_u||^2)\n",
    "$$\n",
    "\n",
    "Typically the optimization done with gradient descent. To apply it we need to first find the partial derivative of the cost function with respect to latent variables which we will denote as $q_{fi}$ and $p_{fu}$. We can find the partial derivative as: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial p_{ku}}=-\\sum_{i \\in I_u}2(r_{ui} - q^T_ip_u)q_{ki} + 2\\lambda p_{ku}\n",
    "$$\n",
    "\n",
    "For **stochastic gradient descent** the update rule for the the $p_u$ vector for a single training example is:\n",
    "\n",
    "$$\n",
    "p_u = p_u + \\alpha ((r_{ui} - q^T_ip_u)q_{i} - \\lambda p_{u})\n",
    "$$\n",
    "\n",
    "similarly for $q_i$ vector we have:\n",
    "\n",
    "$$\n",
    "q_i = q_i + \\alpha ((r_{ui} - q^T_ip_u)p_{u} - \\lambda q_{u})\n",
    "$$\n",
    "\n",
    "For **batch gradient descent** the update rule for the the $p_u$ vector for all preferences where user $u$ appears:\n",
    "\n",
    "$$\n",
    "p_u = p_u + \\alpha (\\sum_{i \\in I_u}(r_{ui} - q^T_ip_u)q_{i} - \\lambda p_{u})\n",
    "$$\n",
    "\n",
    "similarly for $q_i$ vector we have:\n",
    "\n",
    "$$\n",
    "q_i = q_i + \\alpha (\\sum_{u \\in U_i} (r_{ui} - q^T_ip_u)p_{u} - \\lambda q_{u})\n",
    "$$\n",
    "\n",
    "In the above equations $I_u$ is the set of items rated by user $u$ and $U_i$ is the set of users who rated item $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "84apIe07Sn9q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import csr_matrix\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzo9HTNYSn9r"
   },
   "source": [
    "# Movielens dataset\n",
    "\n",
    "We will use the smallest Movielens 100k Dataset which includes 100k preferences. A preference is a triple (user, item, rating). You can download this data set from\n",
    "[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "Note the sparsity of the dataset which shows that most of the user/item matrix is empty. This is a typical property of the datasets in this domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1663599210587,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "JvZEZ2KFSn9r",
    "outputId": "b26ea5cb-8036-42b4-8243-1874704336ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefs = pd.read_csv(\"/home/tevfik/Documents/datasets/ml-latest-small/ratings.csv\", sep=\",\")\n",
    "#prefs = pd.read_csv(\"ratings.csv\", sep=\",\")\n",
    "#prefs = pd.read_csv(\"drive/MyDrive/PycharmProjects/datasets/ml-latest-small/ratings.csv\", sep=\",\")\n",
    "\n",
    "prefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1663599222262,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "KNUNaoQSSn9s",
    "outputId": "244822bf-72ab-4131-fec1-f0ad8ef0a30f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 610\n",
      "Number of items: 9724\n",
      "Number of preferences: 100836\n",
      "Sparsity: 0.016999683055613623\n"
     ]
    }
   ],
   "source": [
    "n_users = prefs.iloc[:,0].unique().size\n",
    "n_items = prefs.iloc[:,1].unique().size\n",
    "n_prefs = prefs.iloc[:,1].size\n",
    "n_factors = 5\n",
    "users = prefs.iloc[:,0].unique()\n",
    "items = prefs.iloc[:,1].unique()\n",
    "\n",
    "print(\"Number of users:\",n_users)\n",
    "print(\"Number of items:\",n_items)\n",
    "print(\"Number of preferences:\",n_prefs)\n",
    "print(\"Sparsity:\",n_prefs/(n_users*n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A6dD4keSn9t"
   },
   "source": [
    "### Error Function\n",
    "\n",
    "Error is calculated by predicting the rating of a user and an item in the test set using the factor representations of users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R6xzCHN4Sn9t"
   },
   "outputs": [],
   "source": [
    "def calc_error(X, u_factors, i_factors):\n",
    "    error = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        u_idx = X.iloc[i,0]\n",
    "        i_idx = X.iloc[i,1]\n",
    "        error += np.abs(X.iloc[i,2] - np.dot(u_factors[u_idx].T, i_factors[i_idx]))\n",
    "    return error/X.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE8UpWc1Sn9u"
   },
   "source": [
    "### Random Predictor Error\n",
    "\n",
    "**Exercise**: What is the expected error of a random predictor given that the actual ratings are uniformly distributed between 1 and 5?\n",
    "\n",
    "Below is a function for calculating this error experimentally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NX4-hb8vSn9v"
   },
   "outputs": [],
   "source": [
    "def random_predictor_error(X):\n",
    "    error = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        u_idx = X.iloc[i,0]\n",
    "        i_idx = X.iloc[i,1]\n",
    "        error += np.abs(X.iloc[i,2] - np.random.randint(1,6))\n",
    "    return error/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bvgyC1VJSn9v"
   },
   "outputs": [],
   "source": [
    "# initialize factor matrices\n",
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1663511703257,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "tN1NtOQISn9w",
    "outputId": "3187371e-4586-4f1d-fcf4-79cfd0388a3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36164631],\n",
       "       [ 0.42188535],\n",
       "       [-0.35970583],\n",
       "       [ 0.12828804],\n",
       "       [-0.29365618]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23479,
     "status": "ok",
     "timestamp": 1663511728591,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "qAVXvpFuSn9w",
    "outputId": "65041189-2e4f-4aad-c3f4-408684a7c4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random predictor error:  1.4802749018207784\n"
     ]
    }
   ],
   "source": [
    "print(\"Random predictor error: \", random_predictor_error(prefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhCmDB3TSn9x"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Following is the stochastic gradient algorithm which is popularized by [Simon Funk](https://sifter.org/simon/journal/20061211.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "executionInfo": {
     "elapsed": 88951,
     "status": "error",
     "timestamp": 1663511961996,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "_km686P-Sn9x",
    "outputId": "f1214f6d-aaad-4ef7-9c67-0b4b4f4a742c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error:  [[3.50335267]]\n",
      "Iteration  0\n",
      "Train error:  [[1.04268217]]\n",
      "Test error:  [[1.14051783]]\n",
      "Iteration  1\n",
      "Train error:  [[0.79683427]]\n",
      "Test error:  [[0.91920941]]\n",
      "Iteration  2\n",
      "Train error:  [[0.73392505]]\n",
      "Test error:  [[0.87012504]]\n",
      "Iteration  3\n",
      "Train error:  [[0.69587005]]\n",
      "Test error:  [[0.84239225]]\n",
      "Iteration  4\n",
      "Train error:  [[0.67357028]]\n",
      "Test error:  [[0.82959776]]\n",
      "Iteration  5\n",
      "Train error:  [[0.66130971]]\n",
      "Test error:  [[0.82305391]]\n",
      "Iteration  6\n",
      "Train error:  [[0.6431522]]\n",
      "Test error:  [[0.81479566]]\n",
      "Iteration  7\n",
      "Train error:  [[0.63485621]]\n",
      "Test error:  [[0.81110643]]\n",
      "Iteration  8\n",
      "Train error:  [[0.62363265]]\n",
      "Test error:  [[0.80681588]]\n",
      "Iteration  9\n",
      "Train error:  [[0.61100393]]\n",
      "Test error:  [[0.79895925]]\n"
     ]
    }
   ],
   "source": [
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "# Stochastic Gradient descent\n",
    "alpha = 0.03\n",
    "my_lambda = 0.1\n",
    "n_iters = 10\n",
    "    \n",
    "print(\"Initial error: \", calc_error(X_train, user_factors, item_factors))\n",
    "\n",
    "for t in range(n_iters):\n",
    "    #q = 10\n",
    "    #ux = prefs.iloc[q,0]; ix = prefs.iloc[q,1]\n",
    "    #print(\"user factor: \",user_factors[ux],\"item factor: \", item_factors[ix])\n",
    "    #print(\"actual rating: \", prefs.iloc[q,2], \"predicted rating: \", np.dot(user_factors[ux].T,item_factors[ix]))\n",
    "    X_train = shuffle(X_train)\n",
    "    for r in range(X_train.shape[0]):\n",
    "        u = X_train.iloc[r,0]\n",
    "        i = X_train.iloc[r,1]\n",
    "        error = X_train.iloc[r,2] - np.dot(user_factors[u].T, item_factors[i])[0,0]\n",
    "        user_factors[u] = user_factors[u] + alpha*(error*item_factors[i] - my_lambda*user_factors[u])\n",
    "        item_factors[i] = item_factors[i] + alpha*(error*user_factors[u] - my_lambda*item_factors[i])  \n",
    "       \n",
    "          \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train, user_factors, item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test, user_factors, item_factors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeaazvPFSn9x",
    "outputId": "dd33ecd6-a3f8-4282-80c9-ed82282505a9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_prefs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9890e4dead9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mitem_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muser_factors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_prefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0muser_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mitem_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_prefs' is not defined"
     ]
    }
   ],
   "source": [
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for r in range(n_prefs):\n",
    "    user_factors[prefs.iloc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "# Stochastic Gradient descent\n",
    "alpha = 0.03\n",
    "my_lambda = 0.1\n",
    "n_iters = 10\n",
    "    \n",
    "print(\"Initial error: \", calc_error(X_train, user_factors, item_factors))\n",
    "\n",
    "for t in range(n_iters):\n",
    "    #q = 10\n",
    "    #ux = prefs.iloc[q,0]; ix = prefs.iloc[q,1]\n",
    "    #print(\"user factor: \",user_factors[ux],\"item factor: \", item_factors[ix])\n",
    "    #print(\"actual rating: \", prefs.iloc[q,2], \"predicted rating: \", np.dot(user_factors[ux].T,item_factors[ix]))\n",
    "    X_train = shuffle(X_train)\n",
    "    for r in range(X_train.shape[0]):\n",
    "        u = X_train.iloc[r,0]\n",
    "        i = X_train.iloc[r,1]\n",
    "        error = X_train.iloc[r,2] - np.dot(user_factors[u].T, item_factors[i])[0,0]\n",
    "        user_factors[u] = user_factors[u] + alpha*(error*item_factors[i] - my_lambda*user_factors[u])\n",
    "        item_factors[i] = item_factors[i] + alpha*(error*user_factors[u] - my_lambda*item_factors[i])  \n",
    "       \n",
    "          \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train, user_factors, item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test, user_factors, item_factors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wQreOX1fvBX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgGw4j4GSn9y"
   },
   "source": [
    "### Batch Gradient Descent\n",
    "If you run the code below you will see that both training and test errors decrease very slowly. Eventually there will be convergence but compared to stochastic version it will be very slow. It is a good example to show the speed advantage of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "executionInfo": {
     "elapsed": 150922,
     "status": "error",
     "timestamp": 1663512327284,
     "user": {
      "displayName": "Tevfik Aytekin",
      "userId": "03705756795675396046"
     },
     "user_tz": -180
    },
    "id": "MdUJbtKBSn9y",
    "outputId": "30deb2e4-31ee-4bbe-889e-023e6b47aa3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "Train error:  [[3.49525882]]\n",
      "Test error:  [[3.4921889]]\n",
      "Iteration  1\n",
      "Train error:  [[3.48792898]]\n",
      "Test error:  [[3.49084991]]\n",
      "Iteration  2\n",
      "Train error:  [[3.47901582]]\n",
      "Test error:  [[3.48828079]]\n",
      "Iteration  3\n",
      "Train error:  [[3.4668294]]\n",
      "Test error:  [[3.48325455]]\n",
      "Iteration  4\n",
      "Train error:  [[3.4486309]]\n",
      "Test error:  [[3.47350457]]\n",
      "Iteration  5\n",
      "Train error:  [[3.41975339]]\n",
      "Test error:  [[3.45485535]]\n",
      "Iteration  6\n",
      "Train error:  [[3.37211439]]\n",
      "Test error:  [[3.4195995]]\n",
      "Iteration  7\n",
      "Train error:  [[3.29190337]]\n",
      "Test error:  [[3.35402093]]\n",
      "Iteration  8\n",
      "Train error:  [[3.15673699]]\n",
      "Test error:  [[3.23587782]]\n",
      "Iteration  9\n",
      "Train error:  [[2.93489331]]\n",
      "Test error:  [[3.03297666]]\n",
      "Iteration  10\n",
      "Train error:  [[2.59442564]]\n",
      "Test error:  [[2.71280924]]\n",
      "Iteration  11\n",
      "Train error:  [[2.13325215]]\n",
      "Test error:  [[2.27075599]]\n",
      "Iteration  12\n",
      "Train error:  [[1.62006561]]\n",
      "Test error:  [[1.77410793]]\n",
      "Iteration  13\n",
      "Train error:  [[1.17997467]]\n",
      "Test error:  [[1.34785087]]\n",
      "Iteration  14\n",
      "Train error:  [[0.89559872]]\n",
      "Test error:  [[1.07293262]]\n",
      "Iteration  15\n",
      "Train error:  [[0.74817501]]\n",
      "Test error:  [[0.93206859]]\n",
      "Iteration  16\n",
      "Train error:  [[0.67760316]]\n",
      "Test error:  [[0.86655206]]\n",
      "Iteration  17\n",
      "Train error:  [[0.64312674]]\n",
      "Test error:  [[0.83512095]]\n",
      "Iteration  18\n",
      "Train error:  [[0.62538144]]\n",
      "Test error:  [[0.81931101]]\n",
      "Iteration  19\n",
      "Train error:  [[0.61590654]]\n",
      "Test error:  [[0.81080826]]\n",
      "Iteration  20\n",
      "Train error:  [[0.61065697]]\n",
      "Test error:  [[0.80589723]]\n",
      "Iteration  21\n",
      "Train error:  [[0.60757612]]\n",
      "Test error:  [[0.80290663]]\n",
      "Iteration  22\n",
      "Train error:  [[0.60557445]]\n",
      "Test error:  [[0.80086643]]\n",
      "Iteration  23\n",
      "Train error:  [[0.60413796]]\n",
      "Test error:  [[0.79937401]]\n",
      "Iteration  24\n",
      "Train error:  [[0.60301587]]\n",
      "Test error:  [[0.7981782]]\n",
      "Iteration  25\n",
      "Train error:  [[0.60207949]]\n",
      "Test error:  [[0.79717387]]\n",
      "Iteration  26\n",
      "Train error:  [[0.60126569]]\n",
      "Test error:  [[0.79626992]]\n",
      "Iteration  27\n",
      "Train error:  [[0.60053967]]\n",
      "Test error:  [[0.79544821]]\n",
      "Iteration  28\n",
      "Train error:  [[0.59987784]]\n",
      "Test error:  [[0.79470568]]\n",
      "Iteration  29\n",
      "Train error:  [[0.59926931]]\n",
      "Test error:  [[0.79402143]]\n",
      "Iteration  30\n",
      "Train error:  [[0.59870691]]\n",
      "Test error:  [[0.79338365]]\n",
      "Iteration  31\n",
      "Train error:  [[0.59818161]]\n",
      "Test error:  [[0.79278942]]\n",
      "Iteration  32\n",
      "Train error:  [[0.59768973]]\n",
      "Test error:  [[0.79222994]]\n",
      "Iteration  33\n",
      "Train error:  [[0.59722727]]\n",
      "Test error:  [[0.79170448]]\n",
      "Iteration  34\n",
      "Train error:  [[0.59679138]]\n",
      "Test error:  [[0.79120215]]\n",
      "Iteration  35\n",
      "Train error:  [[0.59638057]]\n",
      "Test error:  [[0.79072422]]\n",
      "Iteration  36\n",
      "Train error:  [[0.59599358]]\n",
      "Test error:  [[0.79027228]]\n",
      "Iteration  37\n",
      "Train error:  [[0.59562686]]\n",
      "Test error:  [[0.78984478]]\n",
      "Iteration  38\n",
      "Train error:  [[0.59527881]]\n",
      "Test error:  [[0.78943687]]\n",
      "Iteration  39\n",
      "Train error:  [[0.59494732]]\n",
      "Test error:  [[0.78904726]]\n",
      "Iteration  40\n",
      "Train error:  [[0.59463082]]\n",
      "Test error:  [[0.78867481]]\n",
      "Iteration  41\n",
      "Train error:  [[0.59432823]]\n",
      "Test error:  [[0.7883178]]\n",
      "Iteration  42\n",
      "Train error:  [[0.59403822]]\n",
      "Test error:  [[0.78797662]]\n",
      "Iteration  43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-14ba239714b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train error: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test error: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_factors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4c7d1e9f35d3>\u001b[0m in \u001b[0;36mcalc_error\u001b[0;34m(X, u_factors, i_factors)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mu_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mi_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_factors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exctype, excinst, exctb)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexcinst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexctb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;31m# Unlike isinstance and issubclass, CPython exception handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# currently only looks at the concrete type hierarchy (ignoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import copy\n",
    "#initialize factor matrices\n",
    "item_factors = {}\n",
    "user_factors = {}\n",
    "for i in range(n_prefs):\n",
    "    user_factors[prefs.iloc[i,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "    item_factors[prefs.iloc[i,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "    \n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "train_users = X_train.iloc[:,0].unique()\n",
    "train_items = X_train.iloc[:,1].unique()\n",
    "R = csr_matrix((X_train.iloc[:,2], (X_train.iloc[:,0],X_train.iloc[:,1])))\n",
    "\n",
    "# Batch Gradient descent\n",
    "alpha = 0.1\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    for u in train_users:\n",
    "        I_u = X_train[X_train.userId==u].iloc[:,1]\n",
    "        sum_total = 0\n",
    "        for i in I_u:\n",
    "            sum_total += (R[u,i] - np.dot(item_factors[i].T, user_factors[u])[0,0])*item_factors[i]\n",
    "        sum_total = sum_total / I_u.size\n",
    "        user_factors[u] = user_factors[u] + alpha*(sum_total - my_lambda*user_factors[u])\n",
    "    for i in train_items:\n",
    "        U_i = X_train[X_train.movieId==i].iloc[:,0]\n",
    "        sum_total = 0\n",
    "        for u in U_i:\n",
    "            sum_total += (R[u,i] - np.dot(item_factors[i].T, user_factors[u])[0,0])*user_factors[u]\n",
    "        sum_total = sum_total / U_i.size\n",
    "        item_factors[i] = item_factors[i] + alpha*(sum_total - my_lambda*item_factors[i])\n",
    "        \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train,user_factors,item_factors))\n",
    "    print(\"Test error: \", calc_error(X_test,user_factors,item_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcgyM8a9Sn9y"
   },
   "source": [
    "### Question\n",
    "\n",
    "Suppose that user A appears in 200 rows in the user-item preferences dataset. In a single epoch how many updates will there be to the latent vector $p_u$ in stochastic GD vs. Batch GD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOwthAvMSn9y"
   },
   "source": [
    "### Alternating Least Squares (ALS)\n",
    "Since the datasets in this domain are really large, people always try to find ways to speed up the optimization processes. One popular algorithm is called ALS. Here the basic idea is that although the cost function is not convex, when either user factors or items factors are fixed then it becomes a convex function which can be directly solved. The algorithm alternates between updating user and item factors. ([Large-scale parallel collaborative filtering for the netflix prize](https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32)). \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\Theta)}{\\partial p_{ku}}=-\\sum_{i \\in I_u}2(r_{ui} - q^T_ip_u)q_{ki} + 2\\lambda p_{ku} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I_u} q_{ki}q^T_ip_u + \\lambda p_{ku} = \\sum_{i \\in I_u}q_{ki}r_{ui} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i \\in I_u} q_{i}q^T_ip_u + \\lambda p_{u} = \\sum_{i \\in I_u}q_{i}r_{ui} \n",
    "$$\n",
    "\n",
    "$$\n",
    "(Q_{I_u}Q_{I_u}^T + \\lambda I) p_{u} = Q_{I_u}R^T(u,I_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{u} = (Q_{I_u}Q_{I_u}^T + \\lambda I)^{-1}Q_{I_u}R^T(u,I_u)\n",
    "$$\n",
    "\n",
    "where $I$ is the $f × f$ identity matrix. $Q_{I_u}$ denotes the sub-matrix of $Q$ where columns $j \\in I_u$ are selected, and $R^T(u,I_u)$ is the row vector where columns $j \\in I_u$ of the $u$-th row of $R$ are selected.\n",
    "\n",
    "Similarly for $q_i$ we have:\n",
    "\n",
    "$$\n",
    "q_{i} = (P_{U_i}P_{U_i}^T + \\lambda I)^{-1}P_{U_i}R(U_i,i)\n",
    "$$\n",
    "\n",
    "where $I$ is the $f × f$ identity matrix. $P_{U_i}$ denotes the sub-matrix of $P$ where columns $j \\in U_i$ are selected, and $R(U_i,i)$ is the column vector where columns $j \\in U_i$ of the $i$-th column of $R$ are selected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqRNL5UASn9z"
   },
   "outputs": [],
   "source": [
    "#initialize factor matrices\n",
    "Q = pd.DataFrame(np.random.rand(n_factors,n_items)-0.5, columns=items)\n",
    "P = pd.DataFrame(np.random.rand(n_factors,n_users)-0.5, columns=users)\n",
    "\n",
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "\n",
    "train_users = X_train.iloc[:,0].unique()\n",
    "train_items = X_train.iloc[:,1].unique()\n",
    "R = csr_matrix((X_train.iloc[:,2], (X_train.iloc[:,0],X_train.iloc[:,1])))\n",
    "\n",
    "alpha = 0.030\n",
    "my_lambda = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "for t in range(n_iters):\n",
    "    for u in train_users:\n",
    "        I_u = X_train[X_train.userId==u].iloc[:,1]\n",
    "        A = np.dot(Q[I_u],Q[I_u].T)+my_lambda*np.identity(n_factors)\n",
    "        V = np.dot(Q[I_u],R[u,I_u].todense().T)\n",
    "        P[u] = np.dot(np.linalg.inv(A),V)\n",
    "    for i in train_items:\n",
    "        U_i = X_train[X_train.movieId==i].iloc[:,0]\n",
    "        A = np.dot(P[U_i],P[U_i].T)+my_lambda*np.identity(n_factors)\n",
    "        V = np.dot(P[U_i],R[U_i,i].todense())     \n",
    "        Q[i] = np.dot(np.linalg.inv(A),V)\n",
    "        \n",
    "    print(\"Iteration \", t)\n",
    "    print(\"Train error: \", calc_error(X_train,P,Q))\n",
    "    print(\"Test error: \", calc_error(X_test,P,Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7syLd1bcSn9z"
   },
   "outputs": [],
   "source": [
    "A  = X_train[X_train.userId==1].iloc[:,1]\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfgSAsg7Sn9z"
   },
   "outputs": [],
   "source": [
    "R[1,A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXgt5udkSn90"
   },
   "outputs": [],
   "source": [
    "class MF:\n",
    "    \"\"\"\n",
    "    prefs: matrix of prefences, column0=userid, column1=itemid, column2=pref, column3=timestamp \n",
    "    \"\"\"\n",
    "    def __init__(self, prefs, alpha=0.03, mylambda=0.1, n_factors = 10, n_iters = 50):\n",
    "        self.alpha = alpha\n",
    "        self.mylambda = mylambda\n",
    "        self.n_iters = n_iters\n",
    "        self.item_factors = {}\n",
    "        self.user_factors = {}\n",
    "        self.prefs = prefs\n",
    "        # prefs is a matrix containing u, i, r values in each row. This is useful to shuffle and pass over\n",
    "        # the data multiple times in an efficient way in the fit() method.\n",
    "        for r in range(self.prefs.shape[0]):\n",
    "            self.user_factors[self.prefs.loc[r,0]] = np.random.rand(n_factors,1) - 0.5\n",
    "            self.item_factors[self.prefs.iloc[r,1]] = np.random.rand(n_factors,1) - 0.5\n",
    "        print(\"Finished initialization\")\n",
    "        \n",
    "     \n",
    "    def calc_error(self, X):\n",
    "        error = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            u_idx = X[i,0]\n",
    "            i_idx = X[i,1]\n",
    "            error += np.abs(X[i,2] - np.dot(self.user_factors[u_idx].T, self.item_factors[i_idx]))\n",
    "        print(\"Random Predictor Error:\",error/self.prefs.shape[0])\n",
    "        \n",
    "    def fit(self, verbose=False, method=\"SGD\"):\n",
    "        if (method == \"Random\"):\n",
    "            error = 0\n",
    "            for i in range(prefs.shape[0]):\n",
    "                u_idx = prefs.iloc[i,0]\n",
    "                i_idx = prefs.iloc[i,1]\n",
    "                error += np.abs(prefs.iloc[i,2] - np.random.randint(1,6))\n",
    "            return error/prefs.shape[0]\n",
    "            \n",
    "        elif (method == \"SGD\"):\n",
    "            if (verbose): \n",
    "                print(\"Initial error: \", self.calc_error(prefs))                      \n",
    "            for t in range(self.n_iters):\n",
    "                self.prefs = shuffle(self.prefs)\n",
    "                for r in range(self.prefs.shape[0]):\n",
    "                    u = self.prefs.iloc[r,0]\n",
    "                    i = self.prefs.iloc[r,1]\n",
    "                    error = self.prefs.iloc[r,2] - np.dot(self.user_factors[u].T, self.item_factors[i])[0,0]\n",
    "                    self.user_factors[u] = self.user_factors[u] + self.alpha*(error*self.item_factors[i] - self.mylambda*self.user_factors[u])\n",
    "                    self.item_factors[i] = self.item_factors[i] + self.alpha*(error*self.user_factors[u] - self.mylambda*self.item_factors[i])  \n",
    "            \n",
    "                if (verbose): \n",
    "                    print(\"Iteration: \", t)\n",
    "                if (verbose): \n",
    "                    print(\"Train error: \", self.calc_error(self.prefs))                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHyb23UT0QZ5"
   },
   "outputs": [],
   "source": [
    "mf = MF(prefs)\n",
    "mf.fit(verbose=True, method=\"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk47h4VO05Wx"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(prefs, test_size=0.1)\n",
    "mf = MF(X_train, n_iters=3)\n",
    "mf.fit(verbose=True, method=\"SGD\")\n",
    "print(\"Test error: \", mf.calc_error(X_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "329s7qtq3ZQS"
   },
   "outputs": [],
   "source": [
    "prefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuBrKXzppkp4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
