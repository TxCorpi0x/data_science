{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_words = brown.words()\n",
    "brown_text = \" \".join(brown.words()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 57340\n"
     ]
    }
   ],
   "source": [
    "num_sents = len(brown.sents())\n",
    "print(\"number of sentences:\", num_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the fulton county grand jury said friday an investigation of atlanta's recent primary election produ\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, text, window_size=2):\n",
    "        self.text = text.split()\n",
    "        self.word_counts = Counter(self.text)\n",
    "        self.word_list = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        self.word_to_int = {word: idx for idx, word in enumerate(self.word_list)}\n",
    "        self.int_to_word = {idx: word for word, idx in self.word_to_int.items()}\n",
    "        self.data = []\n",
    "        for i, word in enumerate(self.text):\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if j != i and j >= 0 and j < len(self.text):\n",
    "                    self.data.append((self.word_to_int[word], self.word_to_int[self.text[j]]))\n",
    "        self.data = torch.tensor(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_words):\n",
    "        embedded = self.embeddings(input_words)\n",
    "        scores = self.out(embedded)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "Epoch 0, Loss: 6.383079659562927\n",
      "Epoch 10, Loss: 5.099413008034766\n",
      "Epoch 20, Loss: 4.7253947921164405\n",
      "Epoch 30, Loss: 4.472369408285296\n",
      "Epoch 40, Loss: 4.304513565994598\n",
      "Epoch 50, Loss: 4.1911586373373195\n",
      "Epoch 60, Loss: 4.113703016628016\n",
      "Epoch 70, Loss: 4.058059169566846\n",
      "Epoch 80, Loss: 4.015699776711765\n",
      "Epoch 90, Loss: 3.9841774989355794\n"
     ]
    }
   ],
   "source": [
    "# Example corpus\n",
    "corpus =brown_text[:10000]\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "window_size = 2\n",
    "batch_size = 4\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = Word2VecDataset(corpus, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"model loaded\")\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "vocab_size = len(dataset.word_to_int)\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        targets, contexts = data[:, 0], data[:, 1]  # Correctly unpacking data\n",
    "        targets, contexts = targets.to(torch.long), contexts.to(torch.long)  # Ensure correct type\n",
    "        optimizer.zero_grad()\n",
    "        output = model(targets)\n",
    "        loss = criterion(output, contexts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.7393, -0.6095,  0.3763, -0.2495,  3.0213,  1.3814, -0.5919, -0.4371,\n",
      "         0.9403,  0.7485])\n"
     ]
    }
   ],
   "source": [
    "# Inspect embeddings\n",
    "word_embeddings = model.embeddings.weight.data\n",
    "print(word_embeddings[dataset.word_to_int[\"Atlanta's\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(word, word_to_int, int_to_word, embeddings, top_n=5):\n",
    "    # Get the embedding for the given word\n",
    "    word_idx = word_to_int[word]\n",
    "    word_embedding = embeddings[word_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity between this word and all other words in the vocabulary\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings)):\n",
    "        other_word_embedding = embeddings[i].reshape(1, -1)\n",
    "        similarity = cosine_similarity(word_embedding, other_word_embedding)[0][0]\n",
    "        similarities.append((i, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Convert indices back to words and filter out the input word\n",
    "    similar_words = [(int_to_word[sim[0]], sim[1]) for sim in similarities if sim[0] != word_idx]\n",
    "    \n",
    "    # Return the top N most similar words, excluding the word itself\n",
    "    return similar_words[:top_n]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'friday': [('has', 0.7451347), ('had', 0.62919605), ('roads', 0.60290426), ('an', 0.5426837), ('there', 0.4983489)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word = 'friday'  # The word you want to find similar words for\n",
    "similar_words = find_most_similar(word, dataset.word_to_int, dataset.int_to_word, word_embeddings, top_n=5)\n",
    "print(f\"Most similar words to '{word}':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the fulton county grand jury said friday an investigation of atlanta's recent primary election produ\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
