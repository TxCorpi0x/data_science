{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "(by Tevfik Aytekin)\n",
    "\n",
    "Below are some commonly experimental protocols and metrics for evaluating the performance of regression and classification models. Note that there are many other metrics which can be used for evaluation. There is no best metric, instead every metric has some set of properties and should be used according to the specific needs of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment types for estimating the accuracy/error of a model\n",
    "\n",
    "After we train the model on a training set, if we measure accuracy on the training set that will give us an optimistically biased estimate of hypothesis accuracy over future examples. In order to get an **unbiased estimate of future accuracy of our model** we measure the accuracy of our model on a seperate test set. We will look at some of the common train/test split types.\n",
    "\n",
    "### Holdout Method\n",
    "In this method the dataset is divided into two disjoint sets: training and test sets. A model is built using the training set and its performance is evaluated on the test set. Typical percentages of training and test sets can be 80% and 20% respectively. In order to increase the reliability, this method can be repeated which is named random subsampling. One important limitation of this method is that since the training and test sets are formed randomly, some examples might never appear in the training or test set.\n",
    "\n",
    "#### Single Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../datasets/house_prices/train.csv\")\n",
    "\n",
    "\n",
    "X = train.loc[:,['GrLivArea','BedroomAbvGr']]\n",
    "y = train.loc[:,'SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(\"Test MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train.loc[:,['GrLivArea','BedroomAbvGr']]\n",
    "y = train.loc[:,'SalePrice']\n",
    "mae = []\n",
    "for i in range(0,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    print(mae[i])\n",
    "print(\"Test MAE:\", np.mean(mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "In this method the dataset is divided in to $k$ equal sized partitions. The experiment in repeated $k$ times such that in each run one of the partitions in used as a test set and the remaining parititons are used as the training set. Typical values of $k$ are 5 and 10. In the extreme case $k$ can be set to the size of the dataset, this special case is called leave-one-out-cross-validation (LOOCV). LOOCV is used when the dataset size is too small (such as 100) and you don't want to waste your examples by putting them into the test set.\n",
    "\n",
    "#### k-fold cross validation\n",
    "\n",
    "Example of 5-fold cross-validation on a dataset with 10 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"]\n",
    "kf = KFold(n_splits=5, shuffle = True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"%s %s\" % (train_index, test_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOOCV Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"%s %s\" % (train_index, test_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized 5-fold cross-validation on a dataset with 10 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=4, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in ss.split(X):\n",
    "    print(\"%s %s\" % (train_index, test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold CV on house prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train.loc[:,['GrLivArea','BedroomAbvGr']]\n",
    "y = train.loc[:,'SalePrice']\n",
    "mae = []\n",
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds)\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train = X.iloc[train_index,:].fillna(X.iloc[train_index,:].mean())\n",
    "    X_test = X.iloc[test_index,:].fillna(X.iloc[test_index,:].mean())\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "    \n",
    "    lm = linear_model.LinearRegression()\n",
    "    model = lm.fit(X_train, y_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    print(mae[i])\n",
    "    i = i+1\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=kf, n_jobs=-1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "Properties:\n",
    "- Can be used for regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Properties:\n",
    "- Can be used for regression problems\n",
    "- Penalizes large errors more compared to MAE. So, more sensitive to outliers than MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "Properties:\n",
    "- Similar to MSE but scales the error back to the scale of the target values.\n",
    "- Since square root is a nondecreasing function, if a result $a$ is better than $b$ with respect to MSE than $a$ is also better than $b$ with respect to RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Coefficient of Determination, r-squared ($r^2$)\n",
    "\n",
    "$$ r^2 = 1 - \\frac{MSE_{model}}{MSE_{mean}}$$ \n",
    "\n",
    "where $MSE_{mean}$ is the MSE of the baseline model which always predicts the mean of the target values in the training set.\n",
    "\n",
    "Properties:\n",
    "- Evaluates the performance of your model with respect to a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$$MAPE = \\frac{1}{n}\\sum_{i=1}^{n}\\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|*100$$\n",
    "\n",
    "Properties:\n",
    "- Gives a scaled error value independent of the magnitude of the target values.\n",
    "- Problematic when the target value is 0 or very close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_csv(\"../datasets/house_prices/train.csv\")\n",
    "X = train[['GrLivArea']]\n",
    "y = train['SalePrice']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "model_slr = linear_model.LinearRegression()\n",
    "model = model_slr.fit(X_train, y_train)\n",
    "train_predictions = model_slr.predict(X_train)\n",
    "test_predictions = model_slr.predict(X_test)\n",
    "print(\"Test MAE:\", mean_absolute_error(y_test, test_predictions))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, test_predictions))\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, test_predictions)))\n",
    "print(\"Test r-squared:\", r2_score(y_test, test_predictions))\n",
    "print(\"Test MAPE:\",np.mean(np.abs((y_test - test_predictions) / y_test)) * 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "<img src=\"images/confusion_matrix.png\" style=\"max-width:100%; width: 40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TP: the number of cases where both actual and predicted values are positive\n",
    "- FN: the number of cases where actual value is positive but predicted value is negative\n",
    "- FP: the number of cases where actual value is negative but predicted value is positive\n",
    "- TN: the number of cases where both actual and predicted values are negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+FN+FP+TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "$$ Recall = \\frac{TP}{TP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-measure\n",
    "$$ F_1 = \\frac{2*Precision*Recall}{Precision+Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kappa\n",
    "\n",
    "$$ \\kappa = \\frac{p_a-p_e}{1-p_e}$$\n",
    "\n",
    "where $p_a$ is the accuracy of the model and $p_e$ is the expected accuracy of pure chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve and AUC\n",
    "\n",
    "ROC Curves can be used to evaluate the performance of a classifier which produces a probabilistic output for class membership. When the classifier produces a probabilistic output, you have the chance to put a threshold for class membership. ROC curve is generated by moving the threshold to different values and find the true positive rate (tpr) and false positive rate (fpr) at these thresholds. Area under the curve (AUC) is the total area under the ROC curve. Classifiers whose AUC is larger are better classifiers.\n",
    "\n",
    "Below are the definitions that are needed to undertand ROC curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "True \\: Positive\\: Rate\\:(TPR) = Recall = Sensitivity =  \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Specificity = \\frac{TN}{TN+FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "False\\: Positive\\: Rate\\:(FPR) = 1 - Specificity =  \\frac{FP}{FP+TN}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(p, r):\n",
    "    return (2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18000000000000002\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p1 = 0.9\n",
    "r1 = 0.1\n",
    "\n",
    "p2 = 0.5\n",
    "r2 = 0.5\n",
    "print(f_measure(p1,r1))\n",
    "print(f_measure(p2,r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "names = ['age', 'gender', 'cp', 'trestbps', 'chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','diagnosis']\n",
    "heart = pd.read_csv(url, names=names, na_values=[\"?\"])\n",
    "heart.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "X = heart.iloc[:,0:12]\n",
    "y = heart.iloc[:,13]\n",
    "y = y.replace([1,2,3,4],[1,1,1,1])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.03, random_state=3)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = KNeighborsClassifier(n_neighbors=10)\n",
    "clf2 = DecisionTreeClassifier(min_samples_split=20)\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "y_pred1 = clf1.predict_proba(X_test)  \n",
    "y_pred2 = clf2.predict_proba(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the probabilities of being a positive example produced by KNN and DTree models. Given these results which model is better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class values and predicted class probabilities, thresholds, tpr, fpr\n",
    "pd.options.display.float_format = '{:,.1f}'.format\n",
    "results = pd.DataFrame([y_test.values, \n",
    "                y_pred1[:, 1],\n",
    "                y_pred2[:, 1]],\n",
    "                index = ['Actual Class','Prob (KNN)','Prob (DTree)'])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty with evaluating classifiers which produce probabilistic output lies in the fact that there are many possible thredholds we can use. And for every threshold the precision and recall values can change. One way to solve this problem is to calculate precision and recall values for every possible threshold and get a overall result. ROC curves do exactly this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the values with respect to KNN, calculate manually FPR, TPR values for every threshold, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(by =\"Prob (KNN)\", axis=1)\n",
    "results.drop(\"Prob (DTree)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the values with respect to DTree, calculate manually FPR, TPR values for every threshold, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.sort_values(by =\"Prob (DTree)\", axis=1)\n",
    "results.drop(\"Prob (KNN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing easily with roc_curve() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred1[:, 1],drop_intermediate=False)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred2[:, 1],drop_intermediate=False)\n",
    "print(\"fpr1:\",np.round(fpr1,2));\n",
    "print(\"tpr1:\",np.round(tpr1,2));\n",
    "print(\"thresholds1:\",np.round(thresholds1,2));\n",
    "print(\"fpr2:\",np.round(fpr2,2));\n",
    "print(\"tpr2:\",np.round(tpr2,2));\n",
    "print(\"thresholds2:\",np.round(thresholds2,2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "auc1 = auc(fpr1, tpr1)\n",
    "auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "plt.plot(fpr1,tpr1,label='K-NN ROC curve (AUC = %0.2f)' % auc1);\n",
    "plt.plot(fpr2,tpr2,label='DTree ROC curve (AUC = %0.2f)' % auc2);\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above figure, DTree is a better classifier than K-NN since its AUC value is larger. AUC is a measure for comparing the performance of classifiers which produce probabilities for class membership. It is a way to sum up the performance of a classifier for all possible thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below illustrates the two extreme cases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [0,0,0,0,0,1,1,1,1,1]\n",
    "y_pred1 = [0.15,0.1,0.2,0.12,0.18,0.68,0.62,0.92,0.88,0.72]\n",
    "y_pred2 = list(reversed(y_pred1))\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred1,drop_intermediate=False)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred2,drop_intermediate=False)\n",
    "\n",
    "# Seems to be a bug in sklearn, even though in the documentation it says that \n",
    "# thresholds[0] is set to max(y_score) + 1, it is not. So, the following two \n",
    "# lines are needed to start the ROC curve at (0,0).\n",
    "tpr1 = np.r_[0, tpr1] \n",
    "fpr1 = np.r_[0, fpr1] \n",
    "\n",
    "auc1 = auc(fpr1, tpr1)\n",
    "auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "plt.plot(fpr1,tpr1,label='Best ROC curve (AUC = %0.2f)' % auc1);\n",
    "plt.plot(fpr2,tpr2,label='Worst ROC curve (AUC = %0.2f)' % auc2);\n",
    "plt.legend(loc=\"center\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also compare the following cases, which one is a better classifier? In the test set there are 5 positive (green color) and 5 negative (blue color) examples. The arrays show the examples as sorted by predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1\n",
    "y_test = np.array([0,0,0,0,0,1,1,1,1,1])\n",
    "y_pred1 = np.array([0.15,0.1,0.2,0.12,0.18,0.68,0.62,0.92,0.88,0.72])\n",
    "sort_by_prob = y_test[y_pred1.argsort()]\n",
    "plt.figure(figsize=(5,0.5))\n",
    "plt.pcolor(sort_by_prob.reshape(1,10), edgecolors='k',  linewidths=2, cmap='winter')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2\n",
    "y_pred2 = np.array([0.63,0.1,0.2,0.12,0.18,0.68,0.62,0.92,0.88,0.72])\n",
    "sort_by_prob = y_test[y_pred2.argsort()]\n",
    "plt.figure(figsize=(5,0.5))\n",
    "plt.pcolor(sort_by_prob.reshape(1,10), edgecolors='k',  linewidths=2, cmap='winter')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3\n",
    "y_pred3 = np.array([0.69,0.1,0.2,0.12,0.18,0.68,0.62,0.92,0.88,0.72])\n",
    "sort_by_prob = y_test[y_pred3.argsort()]\n",
    "plt.figure(figsize=(5,0.5))\n",
    "plt.pcolor(sort_by_prob.reshape(1,10), edgecolors='k',  linewidths=2, cmap='winter')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the more blue's to the left the better the classifier. AUC score makes this vague characterization precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred1,drop_intermediate=False)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred2,drop_intermediate=False)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred3,drop_intermediate=False)\n",
    "\n",
    "\n",
    "auc1 = auc(fpr1, tpr1)\n",
    "auc2 = auc(fpr2, tpr2)\n",
    "auc3 = auc(fpr3, tpr3)\n",
    "\n",
    "plt.plot(fpr1,tpr1,label='Case1 ROC curve (AUC = %0.2f)' % auc1);\n",
    "plt.plot(fpr2,tpr2,label='Case2 ROC curve (AUC = %0.2f)' % auc2);\n",
    "plt.plot(fpr3,tpr3,label='Case3 ROC curve (AUC = %0.2f)' % auc3);\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1</b>: Can there be two different sequences of positive and negative examples such that their AUC values are the same?\n",
    "\n",
    "<b>Question 2</b>: What are the disadvantages of ROC-curves? One disadvantage is false positives and false negative have the same misclassification costs, but in reality they can be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4\n",
    "y_pred4 = np.array([0.66,0.1,0.2,0.67,0.18,0.68,0.62,0.92,0.88,0.72])\n",
    "sort_by_prob = y_test[y_pred4.argsort()]\n",
    "plt.figure(figsize=(5,0.5))\n",
    "plt.pcolor(sort_by_prob.reshape(1,10), edgecolors='k',  linewidths=2, cmap='winter')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr4, tpr4, thresholds4 = roc_curve(y_test, y_pred4,drop_intermediate=False)\n",
    "\n",
    "auc4 = auc(fpr4, tpr4)\n",
    "\n",
    "plt.plot(fpr3,tpr3,label='Case3 ROC curve (AUC = %0.2f)' % auc3);\n",
    "plt.plot(fpr4,tpr4,label='Case4 ROC curve (AUC = %0.2f)' % auc4);\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
