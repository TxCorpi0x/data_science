{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab7e836",
   "metadata": {},
   "source": [
    "# Linear Regression Theory\n",
    "(by Tevfik Aytekin)\n",
    "\n",
    "Parts of these notes are largely inspired by Andrew Ng's ML course notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411dbf7",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "Assume we are given a data set $D = ((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}))$ \n",
    "\n",
    "where $x^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\mathbb{R}$. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b91998",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Hypothesis (model): \n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta \\in \\mathbb{R}^{n+1}$ is the parameter vector and $x_0=1$. This model assumes that the output is a linear function of the inputs. \n",
    "\n",
    "Cost function:\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}) )^2 \n",
    "\\end{equation}\n",
    "\n",
    "The objective is to find the $\\theta$ values which minimizes the cost.\n",
    "\n",
    "#### An Example\n",
    "Suppose that we have the following toy dataset.\n",
    "\n",
    "| Rooms       | Area        |  Price |\n",
    "| ----------- | ----------- |--------|\n",
    "| 4      | 120       | 100000|\n",
    "| 3   | 110        | 90000|\n",
    "| 5   | 210  | 150000|\n",
    "\n",
    "\n",
    "Linear model: \n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 rooms + \\theta_2 area\n",
    "$$\n",
    "\n",
    "System of linear equations\n",
    "\n",
    "$ \\theta_0 + \\theta_14 + \\theta_2120 = 100000$ \n",
    "\n",
    "$ \\theta_0 + \\theta_13 + \\theta_2110 = 90000$ \n",
    "\n",
    "$ \\theta_0 + \\theta_15 + \\theta_2210 = 150000$ \n",
    "\n",
    "Find $\\theta$ values which minimizes the error in the above set of linear equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bda446",
   "metadata": {},
   "source": [
    "### Probabilistic Interpretation\n",
    "\n",
    "Assumption:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y^{(i)}&= \\theta_0x^{(i)}_0+\\theta_1x^{(i)}_1+\\theta_2x^{(i)}_2+...+\\theta_nx^{(i)}_n + \\epsilon^{(i)} \\\\\n",
    "& = \\theta^Tx^{(i)} + \\epsilon^{(i)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "That is, the output is a linear function of the input variables plus some noise $\\epsilon^{(i)}$  where $\\epsilon^{(i)} \\sim \\mathcal{N}(0,\\,\\sigma^{2})$ which means that the probability density function of $\\epsilon^{(i)}$  can be written as:\n",
    "$$\n",
    "p(\\epsilon^{(i)}) = \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right)\n",
    "$$\n",
    "\n",
    "which implies\n",
    "$$\n",
    "p(y^{(i)} | x^{(i)}; \\theta) = \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right)\n",
    "$$\n",
    "\n",
    "We can understand the above formula as follows: It is the probability of seeing a $y^{(i)}$ given that we see a $x^{(i)}$ and this probability is the value of the normal distribution at $y^{(i)}-\\theta^Tx^{(i)}$.\n",
    "\n",
    "Here is the next step: what is the probability of seeing $n$ number of $y$'s, namely, $y^{(1)}, y^{(2)}, ..., y^{(n)}$ given the corresponding $x$'s, namely, $x^{(1)}, x^{(2)}, ..., x^{(n)}$ . Given that $y^{(i)}$'s are independent (independence assumption) of each other this probability can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\theta)& =\\prod_{i=1}^m p(y^{(i)}| x^{(i)} \\theta) \\\\\n",
    "& = \\prod_{i=1}^m \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "$L(\\theta)$ is known as the likelihood function, that is, it is the probability of seeing the data as a function of the parameters.\n",
    "\n",
    "The next question is to ask which values of $\\theta$ makes this likelihood most likely (known as the maximum likelihood estimation). Now, we have an optimization problem, find the values $\\theta$ which maximizes $L(\\theta)$.\n",
    "\n",
    "A common trick is to maximize the log of this likelihood which is easier to solve and since log is a strictly increasing function the result will be the same.\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "logL(\\theta) & = log\\prod_{i=1}^m \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right) \\\\\n",
    "& = \\sum_{i=1}^m log\\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right) \\\\\n",
    "& = mlog\\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}-\\frac{1}{\\sigma^2}\\frac{1}{2}\\sum_{i=1}^m (y^{(i)} - \\theta^Tx^{(i)})^2 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "As can be seen above, maximizing $logL(\\theta)$ is equivalent to minimizing \n",
    "$$\n",
    "\\frac{1}{2}\\sum_{i=1}^m (y^{(i)} - \\theta^Tx^{(i)})^2 \n",
    "$$\n",
    "which is the cost function we defined at the beginning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e443fc",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent\n",
    "<blockquote>\n",
    "<b>Algorithm</b>: Batch Gradient Descent <br>\n",
    "repeat <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $ <br>\n",
    "until convergence\n",
    "</blockquote>\n",
    "    \n",
    "Below is the derivative of the cost function for a data set where there is a single example ($x, y$).\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta) & =  \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}(y-h_\\theta(x))^2 \\\\\n",
    " & =2\\frac{1}{2}(y-h_\\theta(x)) \\frac{\\partial}{\\partial \\theta_j} (y-h_\\theta(x))\\\\\n",
    " & =(y-h_\\theta(x)) \\frac{\\partial}{\\partial \\theta_j}\\left(y- \\sum_{i=0}^{n}\\theta_ix_{i}\\right)\\\\\n",
    "  & =-(y-h_\\theta(x)) x_{j}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "For $m$ examples:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)})) x_{j}\n",
    "\\end{equation}\n",
    "\n",
    "So, gradient descent algorithm becomes:\n",
    "\n",
    "<blockquote>\n",
    "<b>Algorithm</b>: Batch Gradient Descent <br>\n",
    "repeat<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_j := \\theta_j + \\alpha \\sum\\limits_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)})) x_{j} $ &nbsp;&nbsp;&nbsp;&nbsp; {(for every $j$)} <br>\n",
    "until convergence\n",
    "</blockquote>\n",
    "\n",
    "    \n",
    "$\\alpha$ is called the learning rate which controls the magnitude of the updates. Note that you need to update $\\theta_j$'s simultaneously. \n",
    "\n",
    "### Stochastic Gradient descent\n",
    "\n",
    "<blockquote>\n",
    "<b>Algorithm</b>: Stochastic Gradient Descent <br>\n",
    "repeat <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; shuffle the data <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; for $i = 0$ to $m$ do <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;  $\\theta_j := \\theta_j +\\alpha(y^{(i)}-h_\\theta(x^{(i)})) x_{j} $  &nbsp;&nbsp;&nbsp;&nbsp;   (for every $j$) <br>\n",
    "until convergence\n",
    "</blockquote>\n",
    "    \n",
    "    \n",
    "Different from the batch version stochastic gradient ascent update the parameters after seeing every individual example. Stochastic gradient descent achieves faster convergence than the batch version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d8131",
   "metadata": {},
   "source": [
    "### Closed Form Solution\n",
    "\n",
    "Using vector notation we can write the cost function\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\sum_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))^2 \n",
    "\\end{equation}\n",
    "as follows:\n",
    "\\begin{equation}\n",
    "(y - X\\theta)^T(y-X\\theta)\\\\\n",
    "\\end{equation}\n",
    "\n",
    "In order to find the values of $\\theta$ which minimizes the cost function we need to set the derivative to zero and solve for $\\theta$.\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    " \\nabla (y - X\\theta)^T(y-X\\theta) & = 0 \\\\\n",
    "  -2X^T(y-X\\theta) & = 0 \\\\\n",
    "-2X^Ty+2X^TX\\theta & = 0\\\\\n",
    "(X^TX)^{-1}X^TX\\theta & = (X^TX)^{-1}X^Ty \\\\\n",
    "I\\theta & = (X^TX)^{-1}X^Ty \\\\\n",
    "\\theta & = (X^TX)^{-1}X^Ty\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the time complexity of the matrix inverse operation is $O(d)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ca07d",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression\n",
    "#### Ridge Regression \n",
    "\n",
    "Cost function: <br>\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\displaystyle \\frac{1}{2m} \\left[\\sum\\limits_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))^2 + \\lambda\\sum\\limits_{j=1}^n\\theta_j^2\\right] \n",
    "\\end{equation}\n",
    "\n",
    "<b>Algorithm:</b> Gradient Descent for Ridge Regression\n",
    "<blockquote>\n",
    "repeat<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  $\\theta_0 := \\theta_0 + \\alpha \\frac{1}{m}  \\sum\\limits_{i=1}^m (y^{(i)}-h_\\theta(x^{(i)})x_{0}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  $\\theta_j := \\theta_j + \\alpha \\left[ \\frac{1}{m}  \\sum\\limits_{i=1}^m (y^{(i)}-h_\\theta(x^{(i)}))x_{j} - \\frac{\\lambda}{m}\\theta_j \\right]$ &nbsp;&nbsp;&nbsp;&nbsp;  $(j = 1,2,3, ..., n)$\n",
    "</blockquote>\n",
    "\n",
    "<b>Closed form solution:</b><br>\n",
    "\\begin{equation}\n",
    "\\theta = (X^TX+\\lambda I)^{-1}X^Ty\n",
    "\\end{equation}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
