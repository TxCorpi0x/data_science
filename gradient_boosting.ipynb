{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "(by Tevfik Aytekin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import xgboost as xgb\n",
    "\n",
    "def kaggle_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred));\n",
    "#def kaggle_score(y_true,y_pred):\n",
    "#    return np.sqrt(mean_squared_error(np.log(y_true), np.log(y_pred)));\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train = pd.read_csv(\"../datasets/house_prices/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "Run DecisionTreeRegressor on House Prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 25585.058599695585\n",
      "Test MAPE: 14.284285967461813\n",
      "Test Kaggle: 0.20184425748024476\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = DecisionTreeRegressor()\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor\n",
    "Run sklearn's GradientBoostingRegressor on House Prices dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 15761.201621377608\n",
      "Test MAPE: 9.221913205120185\n",
      "Test Kaggle: 0.13066030238011164\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "mae, kaggle, mape = [], [], []\n",
    "for i in range(1,10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "    regr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    test_predictions = regr.predict(X_test)\n",
    "    mae.append(mean_absolute_error(y_test, test_predictions))\n",
    "    mape.append(mean_absolute_percentage_error(y_test, test_predictions))\n",
    "    kaggle.append(kaggle_score(y_test, test_predictions))\n",
    "\n",
    "print(\"Test MAE:\", np.mean(mae))\n",
    "print(\"Test MAPE:\", np.mean(mape))\n",
    "print(\"Test Kaggle:\", np.mean(kaggle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor from scratch\n",
    "Let us write GradientBoostingRegressor from scratch. Note that \"learning_rate\" is also known as \"shrinkage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators = 100, learning_rate = 0.1, max_depth=3):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.learning_rate * model.predict(X)\n",
    "        grads = y - preds\n",
    "        return grads\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.learning_rate * m.predict(X)\n",
    "        return preds\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How learning_rate works\n",
    "\n",
    "learning_rate shrinks the contribution of each tree. This is an important parameter which has significant effect on the error. You can try to set it to 1 and look at the result. Below is a simple illustration:\n",
    "\n",
    "Suppose that we have only 3 models: M1, M2, and M3 <br>\n",
    "y = 10 <br>\n",
    "learning_rate = 1 (no shrinkage)\n",
    "\n",
    "\n",
    "M1.predict = 8 <br>\n",
    "grad = 10 - 8 = 2\n",
    "\n",
    "fit M2 for y = 2 <br>\n",
    "M2.predict = 1.5 <br>\n",
    "grad = 2 - 1.5 = 0.5\n",
    "\n",
    "fit M3 for y = 0.5\n",
    "\n",
    "\n",
    "Prediction:\n",
    "\n",
    "M1.predict(X) + M2.predict(X) + M3.predict(X) \n",
    "\n",
    "-----------------------\n",
    "3 models: M1, M2, and M3 <br>\n",
    "y = 10<br>\n",
    "learning_rate = 0.1 (with shrinkage)\n",
    "\n",
    "M1.predict = 8 <br>\n",
    "grad = 10 - 0.1 x 8 = 9.2\n",
    "\n",
    "fit M2 for y = 9.2<br>\n",
    "M2.predict = 7<br>\n",
    "grad = 9.2 - 0.1 * 7 = 8.5\n",
    "\n",
    "fit M3 for y = 8.5\n",
    "\n",
    "\n",
    "Prediction:\n",
    "\n",
    "0.1 x M1.predict(X) + 0.1 x M2.predict(X) + 0.1 x M3.predict(X) \n",
    "\n",
    "**Question:** Why does shrinkage improve performance significantly?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Run\n",
    "Now let us run our version of GradientBoostingRegressor on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.209333189769762\n",
      "9.128015085561838\n",
      "9.114494728841056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg2 = GradientBoostingRegressor()\n",
    "reg3 = MyGradientBoostingRegressor(learning_rate=0.1, max_depth=3)\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "reg3.fit(X_train, y_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_test, reg1.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg2.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg3.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bank Marketing Dataset from\n",
    "# https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "\n",
    "bank = pd.read_csv(\"../datasets/bank/bank-full.csv\", delimiter = \";\")\n",
    "# print first 5 examples\n",
    "bank.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     39922\n",
       "yes     5289\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "yes    5289\n",
       "no     5289\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_majority = bank[bank.y==\"no\"]\n",
    "bank_minority = bank[bank.y==\"yes\"]\n",
    " \n",
    "# downsample\n",
    "bank_majority_downsampled = resample(bank_majority, \n",
    "                                 replace=False,    \n",
    "                                 n_samples=5289) \n",
    " \n",
    "bank_balanced = pd.concat([bank_minority, bank_majority_downsampled])\n",
    "bank_balanced.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic/Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-6, 6, 0.1)\n",
    "s = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, s)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('s(x)')\n",
    "plt.suptitle(\"Logistic/Sigmoid function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    \n",
    "    def cross_ent(self, y, p):\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "    \n",
    "    def diff(self, y, p):\n",
    "        return y - p\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) \n",
    "    \n",
    "    def __init__(self, n_estimators = 100, learning_rate = 0.1, max_depth=3):\n",
    "        self.models = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def calc_grads(self, model, X, y):    \n",
    "        preds = self.learning_rate * model.predict(X)\n",
    "        grads = self.diff(y, preds)\n",
    "        return grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for m in self.models:\n",
    "            preds += self.learning_rate * m.predict(X)\n",
    "        return np.round(preds)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=2);\n",
    "            if (i == 0):\n",
    "                model.fit(X, y)\n",
    "                grads = self.calc_grads(model, X, y)\n",
    "            else:\n",
    "                model.fit(X, grads)\n",
    "                grads = self.calc_grads(model, X, grads)\n",
    "            self.models.append(model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80       541\n",
      "           1       0.79      0.79      0.79       517\n",
      "\n",
      "    accuracy                           0.79      1058\n",
      "   macro avg       0.79      0.79      0.79      1058\n",
      "weighted avg       0.79      0.79      0.79      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       541\n",
      "           1       0.85      0.88      0.86       517\n",
      "\n",
      "    accuracy                           0.86      1058\n",
      "   macro avg       0.86      0.86      0.86      1058\n",
      "weighted avg       0.86      0.86      0.86      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.85       541\n",
      "           1       0.84      0.86      0.85       517\n",
      "\n",
      "    accuracy                           0.85      1058\n",
      "   macro avg       0.85      0.85      0.85      1058\n",
      "weighted avg       0.85      0.85      0.85      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.408729661308465\n",
      "9.590032858307856\n",
      "10.054722162771407\n",
      "9.108522855360002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "\n",
    "reg1 = DecisionTreeRegressor()\n",
    "reg2 = GradientBoostingRegressor()\n",
    "reg3 = MyGradientBoostingRegressor()\n",
    "reg4 = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "reg1.fit(X_train, y_train)\n",
    "reg2.fit(X_train, y_train)\n",
    "reg3.fit(X_train, y_train)\n",
    "reg4.fit(X_train, y_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_test, reg1.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg2.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg3.predict(X_test)))\n",
    "print(mean_absolute_percentage_error(y_test, reg4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost can handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.620847107737566\n"
     ]
    }
   ],
   "source": [
    "X = house_train.loc[:,'MSSubClass':'SaleCondition']\n",
    "y = house_train.loc[:,'SalePrice']\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "reg4 = xgb.XGBRegressor()\n",
    "reg4.fit(X_train, y_train)\n",
    "print(mean_absolute_percentage_error(y_test, reg4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tevfikaytekin/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:52:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.79       523\n",
      "           1       0.79      0.78      0.79       535\n",
      "\n",
      "    accuracy                           0.79      1058\n",
      "   macro avg       0.79      0.79      0.79      1058\n",
      "weighted avg       0.79      0.79      0.79      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86       523\n",
      "           1       0.86      0.87      0.87       535\n",
      "\n",
      "    accuracy                           0.86      1058\n",
      "   macro avg       0.86      0.86      0.86      1058\n",
      "weighted avg       0.86      0.86      0.86      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84       523\n",
      "           1       0.84      0.83      0.84       535\n",
      "\n",
      "    accuracy                           0.84      1058\n",
      "   macro avg       0.84      0.84      0.84      1058\n",
      "weighted avg       0.84      0.84      0.84      1058\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87       523\n",
      "           1       0.86      0.89      0.88       535\n",
      "\n",
      "    accuracy                           0.87      1058\n",
      "   macro avg       0.87      0.87      0.87      1058\n",
      "weighted avg       0.87      0.87      0.87      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    " \n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = MyGradientBoostingClassifier()\n",
    "clf4 = xgb.XGBClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);\n",
    "clf4.fit(X_train, y_train);\n",
    "\n",
    "y_pred1 = clf1.predict(X_test)  \n",
    "y_pred2 = clf2.predict(X_test) \n",
    "y_pred3 = clf3.predict(X_test)\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "\n",
    "print(classification_report(y_test,y_pred1))\n",
    "print(classification_report(y_test,y_pred2))\n",
    "print(classification_report(y_test,y_pred3))\n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tevfikaytekin/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:54:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       538\n",
      "           1       0.86      0.90      0.88       520\n",
      "\n",
      "    accuracy                           0.88      1058\n",
      "   macro avg       0.88      0.88      0.88      1058\n",
      "weighted avg       0.88      0.88      0.88      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = bank_balanced.loc[:,'age':'poutcome']\n",
    "y = bank_balanced.loc[:,'y']\n",
    "y = y.replace([\"yes\",\"no\"],[1,0])\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "clf4 = xgb.XGBClassifier()\n",
    "clf4.fit(X_train, y_train);\n",
    "y_pred4 = clf4.predict(X_test) \n",
    "print(classification_report(y_test,y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
